---
layout: post
title: "Generative Adversarial Network(s)"
description: ""
date: 22-05-12

---

### What are GAN(s) ?
GAN is a neural network which helps in generating content. The first application of GAN was to generate images, hence in the following blog, we'll talk in that language. However, GAN(s) are not limited to generating only images.

> The "G" in GAN stands for "Generative".

In a GAN, there are 2 different neural networks which are playing a [minimax](2022-05-12-minimax.md) game with each other. One of the network is known as a Generator, denoted by G(z); the other is known as the Differentiator, denoted by D(x). 

> The "A" in GAN stands for "Adversarial", denoting the *adversarial* relationship between the Generator and the Discriminator.

You might have observed the different types of inputs provided to G and D. The input to G is from a fixed sample space taken from a [standard normal distribution](2022-05-12-standard-normal-distribution-rabbit-hole.md). This is denoted by a vector space *z*. The input to D is of two types:
1. The real data that the network is being trained on. This is denoted by *x*.
2. The output of Generator - more on this later.  

**Technical defintion:** GAN is a type of network that tries to understand (learn) the [distribution](2022-05-12-standard-normal-distribution-rabbit-hole.md) of the input data. Once learned the distribution of input data, the network should be able to generate new data having the same distribution.

### Building blocks of GAN
As explained above, GAN is a combination of two networks - **Discriminator** and **Generator**. <br>

**Discriminator:** The aim of the discriminator is to tell whether this data is fake or real. In other words, the discriminator is striving to get better at discriminating at each iteration. This means that:
- When the discriminator is given real data (i.e. D(x)), it would want to be able to tell the input was real data.
- When the discriminator is given false data (i.e. G(D(z))), it would want to be able to tell that the input was fake.

The building blocks of a discriminators are convolutional layer(s), ReLU activations, and [Batch Norm](2022-05-12-batch-norm.md) layers.

**Generator:** The aim of the generator is to generate a data so good that the generator is unable to identify whether the data is real or fake. In other words, the generator is trying to become a better *counterfeiter* with every iteration. <br>
This means when the output of the generator (i.e. G(z)) is fed to a discriminator (i.e. D(G(z))), the discriminator should not be able to tell whether this is a real object or a fake. <br>
The building blocks of a generator are [transpose convolutional layer(s)](2022-05-23-transpose-conv.md), leaky ReLU activations, and [Batch Norm](2022-05-12-batch-norm.md) layers.


### The Loss Function
The Loss expression for a GAN is given below. Remember, this is mathematical expression, **not** the mathematical formula. The expression says to find a G that minimizes the cost function and to find a D that maximises the cost function.

![GAN Loss Expression](/assets/imgs/gan_loss_expression.png)

The first term in the above equation is the expectation corresponding to x belonging to data space, while the second term is the expectation corresponding to z belonging to generative space. The generator tries to minimise the loss, while the discriminator tries to maximise the loss. 

#### Understanding Loss Intuitively
![GAN Loss Expression](/assets/imgs/gan_loss_expression.png) <br>
We'll not go into the reasons behind using log. For now, it should be noted that the reason we use log because we are using binary cross entropy loss; log comes from the defintion of cross-entropy. <br>
Now, looking at the first term on RHS, we can say that the discriminator wants it to be maximum. As this is concerned only with the real data, the generator is not involved in this term.

![GAN Loss first half](/assets/imgs/gan_loss_first_half.png)

We also know that the discriminator wants to maximise the output of D(G(z)) while the generator wants to minimise the same.

![GAN Loss second half 1](/assets/imgs/gan_loss_second_half_1.png) 

In order to have both the terms of the equation to be in consisten direction, we use **1 - D(G(z))**. Now the generator wants to minimise this term while the discriminator wants to maximise this term.

![GAN Loss second half 2](/assets/imgs/gan_loss_second_half_2.png) 

A small post explaining BCE Maths can be found at [Understanding BCE Loss](2022-05-31-understanding-bce-loss.md).

### Training a GAN


1. Follow documentation guidelines as in [Python Documentation Guideliens](2022-05-29-python-code-documentation)


<br>
---

**References:**
1. [A Friendly Introduction to Generative Adversarial Networks (GANs)](https://www.youtube.com/watch?v=8L11aMN5KY8)
2. [Understand the Math and Theory of GANs in ~ 10 minutes](https://youtu.be/J1aG12dLo4I)
3. [Pytorch Tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#implementation)
4. Binary Cross Entropy Loss
	- [Understanding binary cross-entroy/log loss](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)
	- [Why use Binary Cross Entropy for Generator in Adversarial Networks](https://stats.stackexchange.com/questions/242907/why-use-binary-cross-entropy-for-generator-in-adversarial-networks)

	

**Further Reading:**
1. [Visual Information Theory](http://colah.github.io/posts/2015-09-Visual-Information/)
2. [Conditional Probability](https://www.probabilitycourse.com/chapter1/1_4_0_conditional_probability.php)
3. [Should I use a categorical cross-entropy or binary cross-entropy loss for binary predictions?](https://stats.stackexchange.com/questions/260505/should-i-use-a-categorical-cross-entropy-or-binary-cross-entropy-loss-for-binary)
