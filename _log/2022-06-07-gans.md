---
layout: post
title: "Generative Adversarial Network(s)"
description: ""
date: 22-06-07
tags: integral

---

### What are GAN(s) ?
GAN is a neural network which helps in generating content. The first application of GAN was to generate images, hence in the following blog, we'll talk in that language. However, GAN(s) are not limited to generating only images.

> The "G" in GAN stands for "Generative".

In a GAN, there are 2 different neural networks which are playing a [minimax](2022-05-12-minimax.html) game with each other. One of the network is known as a Generator, denoted by G(z); the other is known as the Differentiator, denoted by D(x). 

> The "A" in GAN stands for "Adversarial", denoting the *adversarial* relationship between the Generator and the Discriminator.

You might have observed the different types of inputs provided to G and D. The input to G is from a fixed sample space taken from a [standard normal distribution](2022-05-12-standard-normal-distribution-rabbit-hole.html). This is denoted by a vector space *z*. The input to D is of two types:
1. The real data that the network is being trained on. This is denoted by *x*.
2. The output of Generator - more on this later.  

**Technical defintion:** GAN is a type of network that tries to understand (learn) the [distribution](2022-05-12-standard-normal-distribution-rabbit-hole.html) of the input data. Once learned the distribution of input data, the network should be able to generate new data having the same distribution.

### Building blocks of GAN
As explained above, GAN is a combination of two networks - **Discriminator** and **Generator**. <br>

**Discriminator:** The aim of the discriminator is to tell whether this data is fake or real. In other words, the discriminator is striving to get better at discriminating at each iteration. This means that:
- When the discriminator is given real data (i.e. D(x)), it would want to be able to tell the input was real data.
- When the discriminator is given false data (i.e. D(G(z))), it would want to be able to tell that the input was fake.

The building blocks of a discriminators are convolutional layer(s), ReLU activations, and [Batch Norm](2022-05-12-batch-norm.html) layers.

**Generator:** The aim of the generator is to generate a data so good that the generator is unable to identify whether the data is real or fake. In other words, the generator is trying to become a better *counterfeiter* with every iteration. <br>
This means when the output of the generator (i.e. G(z)) is fed to a discriminator (i.e. D(G(z))), the discriminator should not be able to tell whether this is a real object or a fake. <br>
The building blocks of a generator are [transpose convolutional layer(s)](2022-05-23-transpose-conv.html), leaky ReLU activations, and [Batch Norm](2022-05-12-batch-norm.html) layers.


### The Loss Function
The Loss expression for a GAN is given below. Remember, this is mathematical expression, **not** the mathematical formula. The expression says to find a G that minimizes the cost function and to find a D that maximises the cost function.

![GAN Loss Expression](/assets/imgs/gan_loss_expression.png)

The first term in the above equation is the expectation corresponding to x belonging to data space, while the second term is the expectation corresponding to z belonging to generative space. The generator tries to minimise the loss, while the discriminator tries to maximise the loss. 

#### Understanding Loss Intuitively
![GAN Loss Expression](/assets/imgs/gan_loss_expression.png) <br>
We'll not go into the reasons behind using log. For now, it should be noted that the reason we use log because we are using binary cross entropy loss; log comes from the defintion of cross-entropy. <br>
Now, looking at the first term on RHS, we can say that the discriminator wants it to be maximum. As this is concerned only with the real data, the generator is not involved in this term.

![GAN Loss first half](/assets/imgs/gan_loss_first_half.png)

We also know that the discriminator wants to maximise the output of D(G(z)) while the generator wants to minimise the same.

![GAN Loss second half 1](/assets/imgs/gan_loss_second_half_1.png) 

In order to have both the terms of the equation to be in consisten direction, we use **1 - D(G(z))**. Now the generator wants to minimise this term while the discriminator wants to maximise this term.

![GAN Loss second half 2](/assets/imgs/gan_loss_second_half_2.png) 

A small post explaining BCE Maths can be found at [Understanding BCE Loss](2022-05-31-understanding-bce-loss.html).

### Training a GAN

To train a GAN, we first need to take care of the building blocks, namely:
1. Discriminator
2. Generator
3. Loss function
4. Data Loader

The Discriminator network is a simple binary classifier having the objective to discriminate between fake and real images. We used a combination of Conv2d, BatchNorm2d, and LeakyReLU layers with a sigmoid layer at the end to create the generator network.

The generator network is given an input vector and it's task is to generate an image, a tensor with the dimensions: CxHxW. The generator network is made up of ConvTranspose2d, BatchNorm2d, and ReLU layers, with a tanh activation function at the end. 

Loss is a simple BCE loss function. For Dataloader, we used ImageFolder dataset and Dataloader class.

First of all, we train the discriminator - first by feeding it the real data and then by feeding it the fake data. The output is generated and loss is calculated. After this, we calculate the gradients (by calling loss.backwards) and update the weights/parameters (by calling optimiser step function).

Next, we train the generator - using the same fake image, we ask the discriminator to discriminate again. Having the weights updated, the discriminator may give a different output this time. After receiving output from the discriminator, we calculate the loss again.
An important point to note here is that while claculating loss for generator, we use fake_labels as true value. This is because for the generator, fake data is true data.

The entire code can be found in [GAN](https://github.com/rajattjainn/ML/tree/main/CV/GANs) repository. The code and functions are well-documented and should be easy to understand. I referred to [Pytorch DCGAN tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#implementation) while working on this repo.

### Results
I trained the discriminator and generator for 5 epochs on [Celeb-A](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset. 8 images generated from the trained neural network are shown below:<br>
![DCGAN Generated Images](/assets/imgs/dcgan_generated_images.png)

The loss vs iteration curve looks the following:<br>
![DCGAN Loss Iter curve](/assets/imgs/dcgan_loss_iter.png)

With this, we conclude the blog on GAN. Next, I'll be working on CAAE - Conditional Adversarial AutoEncoder. Given an image and an age label, this algorithm gives you aged or regressed image of the same person.
<br>
---

**References:**
1. [A Friendly Introduction to Generative Adversarial Networks (GANs)](https://www.youtube.com/watch?v=8L11aMN5KY8)
2. [Understand the Math and Theory of GANs in ~ 10 minutes](https://youtu.be/J1aG12dLo4I)
3. [Pytorch Tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#implementation)
4. Binary Cross Entropy Loss
	- [Understanding binary cross-entroy/log loss](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)
	- [Why use Binary Cross Entropy for Generator in Adversarial Networks](https://stats.stackexchange.com/questions/242907/why-use-binary-cross-entropy-for-generator-in-adversarial-networks)

	

**Further Reading:**
1. [Visual Information Theory](http://colah.github.io/posts/2015-09-Visual-Information/)
2. [Conditional Probability](https://www.probabilitycourse.com/chapter1/1_4_0_conditional_probability.php)
3. [Should I use a categorical cross-entropy or binary cross-entropy loss for binary predictions?](https://stats.stackexchange.com/questions/260505/should-i-use-a-categorical-cross-entropy-or-binary-cross-entropy-loss-for-binary)
